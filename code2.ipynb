{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "def compress_with_dwt(data, wavelet='db1', level=1, threshold=0.05, quantization_bits=2):\n",
    "    # Decomposition using DWT\n",
    "    coeffs = pywt.wavedec(data, wavelet, level=level)\n",
    "\n",
    "    # Thresholding (Soft Thresholding in this example)\n",
    "    thresholded_coeffs = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]\n",
    "\n",
    "    # Quantization\n",
    "    quantized_coeffs = [np.round(c * (2 ** quantization_bits)) for c in thresholded_coeffs]\n",
    "\n",
    "    # Encoding (pickle is used for simplicity; you may choose a different encoding method)\n",
    "    encoded_data = pickle.dumps(quantized_coeffs)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_with_dwt(encoded_data, wavelet='db1', level=1, quantization_bits=2, original_shape=None):\n",
    "    # Decoding\n",
    "    quantized_coeffs = pickle.loads(encoded_data)\n",
    "\n",
    "    # Dequantization\n",
    "    thresholded_coeffs = [c / (2 ** quantization_bits) for c in quantized_coeffs]\n",
    "\n",
    "    # Reconstruction\n",
    "    reconstructed_data = pywt.waverec(thresholded_coeffs, wavelet)\n",
    "\n",
    "    # Reshape to the original shape if provided\n",
    "    if original_shape is not None:\n",
    "        reconstructed_data = reconstructed_data.reshape(original_shape)\n",
    "\n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Original Data: 319003017 bytes\n",
      "Size of Compressed Data: 506654646 bytes\n",
      "Compression Ratio: 0.63\n",
      "Number of Rows in Original Data: 108004\n",
      "Number of Columns in Original Data: 140\n",
      "Number of Bytes in Original Data per Row: 2953.62 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'output.csv' is your CSV file with four columns of data\n",
    "    csv_file_path = 'output2.csv'\n",
    "\n",
    "    data = pd.read_csv('output2.csv')\n",
    "\n",
    "    # Set your preferred parameters for the DWT\n",
    "    wavelet_type = 'db2'  # Changed to 'db2' for potentially better compression\n",
    "    decomposition_level = 3  # Increased decomposition level for more compression\n",
    "    threshold_value = 0.01  # Lowered threshold value for potentially better compression\n",
    "    quantization_bits = 2  # Decreased quantization bits for potentially better compression\n",
    "\n",
    "    # Compression using DWT\n",
    "    encoded_result = compress_with_dwt(data, wavelet=wavelet_type, level=decomposition_level,\n",
    "                                       threshold=threshold_value, quantization_bits=quantization_bits)\n",
    "\n",
    "    # Store the original shape for later reconstruction\n",
    "    original_shape = data.shape\n",
    "\n",
    "    # Decompression using DWT\n",
    "    decompressed_result = decompress_with_dwt(encoded_result, wavelet=wavelet_type, level=decomposition_level,\n",
    "                                              quantization_bits=quantization_bits, original_shape=original_shape)\n",
    "\n",
    "    # Convert the decompressed result to a Pandas DataFrame\n",
    "    decompressed_dataframe = pd.DataFrame(decompressed_result, columns=data.columns)\n",
    "\n",
    "    # Save the compressed and decompressed data to new CSV files\n",
    "    pd.DataFrame({'CompressedData': [encoded_result]}).to_csv('compressed_data.csv', index=False)\n",
    "    decompressed_dataframe.to_csv('decompressed_data.csv', index=False)\n",
    "\n",
    "    # Calculate the size of the original and compressed data\n",
    "    original_size = os.path.getsize(csv_file_path)\n",
    "    compressed_size = os.path.getsize('compressed_data.csv')\n",
    "\n",
    "    # Calculate the compression ratio\n",
    "    compression_ratio = original_size / compressed_size\n",
    "\n",
    "    print(f\"Size of Original Data: {original_size} bytes\")\n",
    "    print(f\"Size of Compressed Data: {compressed_size} bytes\")\n",
    "    print(f\"Compression Ratio: {compression_ratio:.2f}\")\n",
    "\n",
    "    # Additional prints for diagnostic purposes\n",
    "    print(f\"Number of Rows in Original Data: {original_shape[0]}\")\n",
    "    print(f\"Number of Columns in Original Data: {original_shape[1]}\")\n",
    "    print(f\"Number of Bytes in Original Data per Row: {original_size / original_shape[0]:.2f} bytes\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
